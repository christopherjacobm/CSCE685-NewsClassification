{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def readData():\n",
    "    news_df = pd.read_csv(\"uci-news-aggregator.csv\")\n",
    "    return news_df\n",
    "\n",
    "def removeStopWords(x):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopSet = set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        wordList = x[i].split(\" \")\n",
    "        cleanLine = [stemmer.stem(word.lower()) for word in wordList if word not in stopSet]\n",
    "        x[i] = ' '.join(cleanLine)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "def splitData(news_df):\n",
    "    x = news_df['TITLE'].values\n",
    "    y = news_df['CATEGORY'].values\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=42)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProbabilityMethodAccuracy(x_train, x_test, y_train, y_test):\n",
    "    categories = ['t' ,'e', 'm', 'b']\n",
    "    \n",
    "    # Get words of each category\n",
    "    categorywise_words = {}\n",
    "    categorywise_words['t'] = set()\n",
    "    categorywise_words['e'] = set()\n",
    "    categorywise_words['m'] = set()\n",
    "    categorywise_words['b'] = set()\n",
    "    \n",
    "    # Stores the number of occurences of each word in that particular category\n",
    "    word_count = {}\n",
    "    word_count['t'] = {}\n",
    "    word_count['e'] = {}\n",
    "    word_count['m'] = {}\n",
    "    word_count['b'] = {}\n",
    "    n = len(x_train)\n",
    "    for i in range(n):\n",
    "        headline = x_train[i].lower().split()\n",
    "        for word in headline:\n",
    "            categorywise_words[y_train[i]].add(word)\n",
    "            cur = word_count[y_train[i]].get(word, 0)\n",
    "            word_count[y_train[i]][word] = cur + 1\n",
    "    \n",
    "    # Number of unique words in each category \n",
    "    category_count = {}\n",
    "    category_count['t'] = len(categorywise_words['t'])\n",
    "    category_count['e'] = len(categorywise_words['e'])\n",
    "    category_count['m'] = len(categorywise_words['m'])\n",
    "    category_count['b'] = len(categorywise_words['b'])\n",
    "    \n",
    "    \n",
    "    y_actual = []\n",
    "    n = len(x_test)\n",
    "    for i in range(n):\n",
    "        headline = x_test[i].lower().split()\n",
    "        probabilities_of_words = []\n",
    "        for word in headline:\n",
    "            probabilities_of_categories = []\n",
    "            # Get probability of each category. The assumption is if a word occurs more frequently in a category, that category is more likely when that word occurs\n",
    "            for category in categories:\n",
    "                _ = 1.0 * word_count[category].get(word, 0) / (1 + word_count['t'].get(word, 0) + \\\n",
    "                                                               word_count['e'].get(word, 0) + \\\n",
    "                                                               word_count['m'].get(word, 0) + \\\n",
    "                                                               word_count['b'].get(word, 0))\n",
    "                probabilities_of_categories.append((category, _))\n",
    "            # Take the highest probability category\n",
    "            _ = max(probabilities_of_categories, key = lambda item:item[1])\n",
    "            probabilities_of_words.append(_)\n",
    "            # For each headline, let the word with the strongest affinity for a category decide the category for the entire headline\n",
    "        _ = max(probabilities_of_words, key = lambda item:item[1])\n",
    "        y_actual.append(_[0])\n",
    "    \n",
    "    y_test = list(y_test)\n",
    "    return accuracy_score(y_actual, y_test)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the probabilistic model 91.08470242886227\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    news_df = readData()\n",
    "    x_train, x_test, y_train, y_test = splitData(news_df)\n",
    "    print \"the probabilistic model\", getProbabilityMethodAccuracy(x_train, x_test, y_train, y_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
